{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries we need and likely a lot we don't need.\n",
    "# We used mostly re, requests, bs4, and selenium in this notebook\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import system\n",
    "from math import floor\n",
    "from copy import deepcopy\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't think I used this but I may have when I used selenium for webscraping Wikipedia\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will start by finding the ebay url for any tv show we call.\n",
    "We then use beautiful soup to pull and parse the data as html text.\n",
    "From there we tell it to search for the class that contains the number of results for our search.\n",
    "Then we clean the returned text as necessary so we grab only the number.\n",
    "Finally, we return a dictionary that labels the number of results as an integer and the name of the show.\n",
    "\"\"\"\n",
    "def ebay_results(tv_name):\n",
    "    results = []\n",
    "    for name in tv_name:\n",
    "        name = name.replace(' ', '+').replace('&','and').lower()\n",
    "        url = \"https://www.ebay.com/sch/i.html?_from=R40&_trksid=m570.l1313&_nkw=\"+name+\"+tv+show&_sacat=0\"\n",
    "        link = requests.get(url)\n",
    "        time.sleep(random.choice([x/10 for x in range(8,14)]))\n",
    "        link_soup = BS(link.content, 'html.parser')\n",
    "        links = link_soup.find(class_='srp-controls__count-heading').text.split()[0].replace(',','')\n",
    "        results.append({'Results' : int(links), 'Show': name.title().replace('+', ' ')})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will start by finding the bonanza url for any tv show we call.\n",
    "We then use beautiful soup to pull and parse the data as html text.\n",
    "From there we tell it to search for the class that contains the number of results for our search.\n",
    "Then we clean the returned text as necessary so we grab only the number.\n",
    "Finally, we return a dictionary that labels the number of results as an integer and the name of the show.\n",
    "\"\"\"\n",
    "def bonanza_results(tv_name):\n",
    "    results = []\n",
    "    for name in tv_name:\n",
    "        name = name.replace(' ', '%20').replace('&', 'and').lower()\n",
    "        url = \"\"\"https://www.bonanza.com/items/search?q[catalog_id]=&q[country_to_filter]=\n",
    "                 US&q[filter_category_id]=&q[in_booth_id]=&q[ship_country]=1&q[shipping_in_price]=\n",
    "                 0&q[sort_by]=relevancy&q[suggestion_found]=&q[translate_term]=true&q[search_term]=\n",
    "                 \"\"\"+name+\"\"\"%20tv%20show\"\"\"\n",
    "        link = requests.get(url)\n",
    "        time.sleep(random.choice([x/10 for x in range(8,14)]))\n",
    "        link_soup = BS(link.content, 'html.parser')\n",
    "        links = link_soup.find(id='listing_count_number').text.replace(',','')\n",
    "        results.append({'Results' : int(links), 'Show': name.title().replace('%20', ' ') })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will start by finding the etsy url for any tv show we call.\n",
    "We then use beautiful soup to pull and parse the data as html text.\n",
    "From there we tell it to search for the class that contains the number of results for our search.\n",
    "Then we clean the returned text as necessary so we grab only the number.\n",
    "Finally, we return a dictionary that labels the number of results as an integer and the name of the show.\n",
    "NOTE: For this one, we needed to tell links that if the class on the page does not exist (meaning no results were found) \n",
    "      then we must replace the 'Results' with 0. This was vital because when there are no results in the search,\n",
    "      the class does not exist in the inspect, hence the if else at the end of links. We also used .replace() twice\n",
    "      because we needed to get rid of both '(' and ',' for the integer to work.\n",
    "\"\"\"\n",
    "def etsy_results(tv_name):\n",
    "    results = []\n",
    "    for name in tv_name:\n",
    "        name = name.replace(' ', '%20').replace('&', 'and').lower()\n",
    "        url = \"\"\"https://www.etsy.com/search?q=\"\"\"+name+\"\"\"%20tv%20show\"\"\"\n",
    "        link = requests.get(url)\n",
    "        time.sleep(random.choice([x/10 for x in range(8,14)]))\n",
    "        link_soup = BS(link.content, 'html.parser')\n",
    "        links = link_soup.find(class_='float-left wt-pt-lg-2 wt-nudge-b-1').text.split()[-2].replace('(', '').replace(',','') if link_soup.find(class_='float-left wt-pt-lg-2 wt-nudge-b-1') else 0\n",
    "        results.append({'Results' : int(links), 'Show': name.title().replace('%20', ' ')})\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the webdriver to open chrome through selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the driver to a specific url and open the browser\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_American_television_programs_currently_in_production#2010s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the whole list of shows from this specific class\n",
    "list_of_shows = driver.find_elements_by_class_name('mw-parser-output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it into a string of text separated by a line\n",
    "# Pull only the show names from 2010 onwards and omit everything after the last show in 2019\n",
    "show = str(list_of_shows[0].text).split('\\n')\n",
    "shows = show[480:1279]\n",
    "shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an empty list which we will then add our shows to\n",
    "shows_2010 = []\n",
    "shows_2011 = []\n",
    "shows_2012 = []\n",
    "shows_2013 = []\n",
    "shows_2014 = []\n",
    "shows_2015 = []\n",
    "shows_2016 = []\n",
    "shows_2017 = []\n",
    "shows_2018 = []\n",
    "shows_2019 = []\n",
    "\"\"\"\n",
    "Write a for loop that will take every show within the year we want.\n",
    "Use .index() to find the exact string we want to start and stop at.\n",
    "Clean the data so we get only the show's name and nothing after (there were parentheses with years that were unnecessary).\n",
    "Append the results to the appropriate list.\n",
    "\"\"\"\n",
    "for show in shows:\n",
    "    if shows.index(show) > shows.index('2010[edit]') and shows.index(show) < shows.index('2011[edit]'):\n",
    "        shows_2010.append(re.sub(\"\\(.+\\)\",'',show)) # used regular expression to get rid of years inside parentheses \n",
    "    elif shows.index(show) > shows.index('2011[edit]') and shows.index(show) < shows.index('2012[edit]'):\n",
    "        shows_2011.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2012[edit]') and shows.index(show) < shows.index('2013[edit]'):\n",
    "        shows_2012.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2013[edit]') and shows.index(show) < shows.index('2014[edit]'):\n",
    "        shows_2013.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2014[edit]') and shows.index(show) < shows.index('2015[edit]'):\n",
    "        shows_2014.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2015[edit]') and shows.index(show) < shows.index('2016[edit]'):\n",
    "        shows_2015.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2016[edit]') and shows.index(show) < shows.index('2017[edit]'):\n",
    "        shows_2016.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2017[edit]') and shows.index(show) < shows.index('2018[edit]'):\n",
    "        shows_2017.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2018[edit]') and shows.index(show) < shows.index('2019[edit]'):\n",
    "        shows_2018.append(re.sub(\"\\(.+\\)\",'',show))\n",
    "    elif shows.index(show) > shows.index('2019[edit]'):\n",
    "        shows_2019.append(re.sub(\"\\(.+\\)\",'',show))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = pd.read_csv('data_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set all decades equal to the appropriate results\n",
    "\"\"\"\n",
    "ebay_1 = pd.DataFrame(ebay_results(shows_2010))\n",
    "ebay_2 = pd.DataFrame(ebay_results(shows_2011))\n",
    "ebay_3 = pd.DataFrame(ebay_results(shows_2012))\n",
    "ebay_4 = pd.DataFrame(ebay_results(shows_2013))\n",
    "ebay_5 = pd.DataFrame(ebay_results(shows_2014))\n",
    "ebay_6 = pd.DataFrame(ebay_results(shows_2015))\n",
    "ebay_7 = pd.DataFrame(ebay_results(shows_2016))\n",
    "ebay_8 = pd.DataFrame(ebay_results(shows_2017))\n",
    "ebay_9 = pd.DataFrame(ebay_results(shows_2018))\n",
    "ebay_10 = pd.DataFrame(ebay_results(shows_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set them all equal to one list\n",
    "\"\"\"\n",
    "ebays = [ebay_1, ebay_2, ebay_3, ebay_4, ebay_5, ebay_6, ebay_7, ebay_8, ebay_9, ebay_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay_results_all = pd.concat(ebays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set a new variable equal to a list of all data we want to pull from filtered_data\n",
    "\"\"\"\n",
    "data_list = []\n",
    "\n",
    "for i in range(len(filtered_data)):\n",
    "    data_list.append((filtered_data.imdb_id[i], int(filtered_data.etsy[i]), int(filtered_data.ebay[i]), \n",
    "                     int(filtered_data.bonanza[i]), filtered_data.title[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will send the data_list to AWS\n",
    "\"\"\"\n",
    "def ship_2_sql(data_list):\n",
    "    cnx = mysql.connector.connect(\n",
    "                                  host = 'flatiron.cqpwzkhqau62.us-east-2.rds.amazonaws.com',\n",
    "                                  user = 'tappel',\n",
    "                                  password = 'Taylor19',\n",
    "                                  database = 'tv_merch')\n",
    "    stmt = \"\"\"INSERT INTO merch_data (show_id, etsy, ebay, bonanza, show_name) \n",
    "              VALUES (%s, %s, %s, %s, %s)\"\"\"\n",
    "    for show in data_list:\n",
    "        print(show)\n",
    "        c.execute(stmt, show)\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set a new variable equal to a list of all data we want to pull from filtered_data\n",
    "\"\"\"\n",
    "show_data_list = []\n",
    "\n",
    "for i in range(len(filtered_data)):\n",
    "    show_data_list.append((filtered_data.imdb_id[i], filtered_data.title[i], str(filtered_data.genre[i]), \n",
    "                         str(filtered_data.rated[i]), str(filtered_data.released[i]), filtered_data.imdb_rating[i],\n",
    "                         str(filtered_data.imdb_votes[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will send the show_data_list to AWS\n",
    "\"\"\"\n",
    "def ship_2_sql2(show_data_list):\n",
    "    cnx = mysql.connector.connect(\n",
    "                                      host = 'flatiron.cqpwzkhqau62.us-east-2.rds.amazonaws.com',\n",
    "                                      user = 'tappel',\n",
    "                                      password = 'Taylor19',\n",
    "                                      database = 'tv_merch')\n",
    "    stmt = \"\"\"INSERT INTO shows (show_id, show_name, genre, rated, released, imdb_rating, imdb_votes) \n",
    "              VALUES (%s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    \n",
    "    \n",
    "    for show in show_data_list:\n",
    "        print(show)\n",
    "        c.execute(stmt, show)\n",
    "cnx.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
